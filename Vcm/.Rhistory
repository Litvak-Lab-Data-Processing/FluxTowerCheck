basedir <- paste0("~/Desktop/FluxTowerCheck/", site, "/")
# Create the directory if it doesn't exist
if (!dir.exists(basedir)) {
dir.create(basedir, recursive = TRUE)
}
setwd(basedir)
## these are for Vcp, add a loop to fetch for other sites - RD
file_ids <- c("15CAusJQddgR3RV3Y0kUwLlmUPpSLGLuV")  # file ID from google drive
file_names <- c(paste0(basedir, "toa5/flux.csv"))
file_ids
flux$TIMESTAMP
print(max(flux$TIMESTAMP, na.rm = T))
## time series from TC1
tc1data <- read.csv("~/Desktop/correcTIR_Rae/processed_20240410/Rae_image_processed_test_TC1.csv")
head(tc1data)
corrected <- colnames(tc1data)[grep("mean_fully_corrected",colnames(tc1data))]
impdata <- colnames(tc1data)[3:9]
# subset dataframe
tc1 <- tc1data %>%
select(all_of(c(impdata, corrected)))
# convert to long to visualize and add labels
tc1long <- tc1 %>%
pivot_longer(
cols = -c(Timestamp, T_air, RH, sky_temp, LW_IN, rho_v, tau),
names_to = "ROI",
values_to = "T_IR"
)
tc1long$species <- ifelse(grepl("pinon", tc1long$ROI), "pinon", "juniper")
# convert timestamp to posix
tc1long$time <- as.POSIXct(tc1long$Timestamp, tz="UTC", format = "%Y-%m-%d %H:%M:%S")
tc1long <- subset(tc1long, T_IR < 100)
timeTC1 <- ggplot(tc1long) +
geom_line(aes(x=time, y=T_IR, color=species)) +
theme_minimal()
ggplotly(timeTC1)
ggsave("~/Desktop/Thermal_Processing_Information/timeseriesTC1_MRD.jpg")
timeTC1_roi <- ggplot(tc1long) +
geom_line(aes(x=time, y=T_IR, color=ROI)) +
theme_minimal()
ggplotly(timeTC1_roi)
# get mean and CI
tc1long_means <- tc1long %>%
group_by(species, time) %>%
mutate( mean = mean(T_IR, na.rm = TRUE),
sd = sd(T_IR, na.rm = TRUE),
n = n(),
se = sd / sqrt(n),
ci_lower = mean - qt(0.975, df = n - 1) * se,
ci_upper = mean + qt(0.975, df = n - 1) * se)
timeTC1 <- ggplot(tc1long_means) +
geom_line(aes(x = time, y = mean, color = species)) +
geom_ribbon(aes(x = time, ymin = ci_lower, ymax = ci_upper, fill = species), alpha = 0.2, color = NA) +
theme_minimal()
## plot daily pattern
head(tc1long)
tc1long$hour <- lubridate::hour(tc1long$time)
tc1long$minute <- lubridate::minute(tc1long$time)
tc1long <- tc1long %>%
mutate(
date = as.Date(time),
hour = hour(time) + minute(time)/60  # fractional hour
)
tc1long <- tc1long %>%
mutate(rounded_time = round_date(time, unit = "15 minutes"))
tc1long <- tc1long %>%
mutate(
hour_15min = hour(rounded_time) + minute(rounded_time) / 60
)
tc1diurnal <- ggplot(tc1long, aes(x = hour_15min, y = T_IR, group = date, color = factor(date))) +
geom_line() +
facet_grid(.~species) +
labs(x = "Hour (15-min bins)", y = "T_IR", color = "Date") +
theme_minimal()
tc1diurnal
ggboxplot(tc1long, x="species", y="T_IR")
library(car)
leveneTest(tc1long$T_IR~tc1long$species)
leveneTest(tc2long$T_IR~tc2long$species)
tc1diurnal
leveneTest(tc1long$T_IR~tc1long$species)
leveneTest(tc2long$T_IR~tc2long$species)
print(max(flux$TIMESTAMP, na.rm = T))
# ENTER YEAR TO CHECK HERE
### Beginning of time takes a really long time to render - have it as a separate script if someone wants to view it.
yeartocheck <- 2025
# ENTER SITE TO CHECK HERE (capital first letter, lowercase after)
site <- "Mpg"
# #basedir <- paste0("C:/FluxTowerData/", site, "/") # for Jakes computer
basedir <- paste0("~/Desktop/FluxTowerTest/", site, "/")
# Create the directory if it doesn't exist
if (!dir.exists(basedir)) {
dir.create(basedir, recursive = TRUE)
}
setwd(basedir)
## these are for Vcp, add a loop to fetch for other sites - RD
file_ids <- c("16Uxw1K1pmymRfRWukxzIO9bpRwKczLOU", "16XQL57HbfswxkVL3EDK3vIUIRR6izntz")  # file ID from google drive
file_names <- c(paste0(basedir, "toa5/flux.csv"), paste0(basedir, "toa5/soil.csv"))
# Create the directory if it doesn't exist
if (!dir.exists(paste0(basedir, "/toa5"))) {
dir.create(paste0(basedir, "/toa5"), recursive = TRUE)
}
# # Loop through file IDs to download
for (i in seq_along(file_ids)) {
file_url <- paste0("https://drive.google.com/uc?id=", file_ids[i])
destfile <- file_names[i]
# Check if the destination file exists, and remove it if so
if (file.exists(destfile)) {
file.remove(destfile)
}
download.file(file_url, destfile, mode = "wb", timeout =300)
}
otus <- read.csv("~/Desktop/HEWITT/2024_Legacy/ReProcess_20250310/OTUS_reassignguild.csv")
tax <- read.csv("~/Desktop/HEWITT/2024_Legacy/ReProcess_20250310/FUNGUILD/tax_expanded_cutoffs_guilds.csv")
tax_sub <- subset(tax, OTU_ID %in% otus$OTUS)
tax_sub
write.csv("~/Desktop/HEWITT/2024_Legacy/ReProcess_20250310/tax_to_reassignGuilds.csv")
write.csv(tax_sub, "~/Desktop/HEWITT/2024_Legacy/ReProcess_20250310/tax_to_reassignGuilds.csv")
max(flux$date, na.rm = T)
print(max(flux$TIMESTAMP, na.rm = T))
## these are for Vcp, add a loop to fetch for other sites - RD
file_ids <- c("16Uxw1K1pmymRfRWukxzIO9bpRwKczLOU", "16XQL57HbfswxkVL3EDK3vIUIRR6izntz")  # file ID from google drive
file_names <- c(paste0(basedir, "toa5/flux.csv"), paste0(basedir, "toa5/soil.csv"))
# Create the directory if it doesn't exist
if (!dir.exists(paste0(basedir, "/toa5"))) {
dir.create(paste0(basedir, "/toa5"), recursive = TRUE)
}
# # Loop through file IDs to download
for (i in seq_along(file_ids)) {
file_url <- paste0("https://drive.google.com/uc?id=", file_ids[i])
destfile <- file_names[i]
# Check if the destination file exists, and remove it if so
if (file.exists(destfile)) {
file.remove(destfile)
}
download.file(file_url, destfile, mode = "wb", timeout =300)
}
# get limit data #update with filters from the processing code
lims <- read.csv(paste0(basedir, "MpgVarLims.csv"))
# ENTER YEAR TO CHECK HERE
### Beginning of time takes a really long time to render - have it as a separate script if someone wants to view it.
yeartocheck <- 2025
# ENTER SITE TO CHECK HERE (capital first letter, lowercase after)
site <- "Mpg"
# #basedir <- paste0("C:/FluxTowerData/", site, "/") # for Jakes computer
basedir <- paste0("~/Desktop/FluxTowerData/", site, "/")
# Create the directory if it doesn't exist
if (!dir.exists(basedir)) {
dir.create(basedir, recursive = TRUE)
}
setwd(basedir)
## these are for Vcp, add a loop to fetch for other sites - RD
file_ids <- c("16Uxw1K1pmymRfRWukxzIO9bpRwKczLOU", "16XQL57HbfswxkVL3EDK3vIUIRR6izntz")  # file ID from google drive
file_names <- c(paste0(basedir, "toa5/flux.csv"), paste0(basedir, "toa5/soil.csv"))
# Create the directory if it doesn't exist
if (!dir.exists(paste0(basedir, "/toa5"))) {
dir.create(paste0(basedir, "/toa5"), recursive = TRUE)
}
# # Loop through file IDs to download
for (i in seq_along(file_ids)) {
file_url <- paste0("https://drive.google.com/uc?id=", file_ids[i])
destfile <- file_names[i]
# Check if the destination file exists, and remove it if so
if (file.exists(destfile)) {
file.remove(destfile)
}
download.file(file_url, destfile, mode = "wb", timeout =300)
}
# import data directly from google drive
flux_colnames <- fread(paste0(basedir,"/toa5/flux.csv"),
header = TRUE, skip=1,sep=",", fill=TRUE,
na.strings=c(-9999,"#NAME?"))[1,]
# fix units
flux_units <- flux_colnames[1,]
flux <- fread(paste0(basedir,"/toa5/flux.csv"),
header = FALSE, skip=2, sep=",", fill=TRUE,
na.strings=c(-9999,"#NAME?"),
col.names=colnames(flux_colnames))
flux <- read.csv(paste0(basedir, "/toa5/flux.csv"), header = FALSE, skip = 4)
colnames(flux) <- colnames(flux_colnames)
flux <- data.table(flux)
#convert to numeric
flux[, 2:ncol(flux) := lapply(.SD, function(x) {
as.numeric(as.character(x))
}), .SDcols = 2:ncol(flux)]
#convert timestamp to posix
flux$TIMESTAMP <- as.POSIXct(flux$TIMESTAMP, format="%Y-%m-%d %H:%M:%S", tz="MST")
flux <- subset(flux, year(flux$TIMESTAMP) %in% yeartocheck) # change here
# create derivative date columns
flux[,':=' (year = year(TIMESTAMP), doy = yday(TIMESTAMP), date = as.Date(TIMESTAMP))]
flux$date_time = ymd_hms(flux$TIMESTAMP)
flux$date <- as.Date.POSIXct(flux$date_time, format="%Y-%m-%d")
# calculate CO2 ppm from CO2_mean (mg/m^3)
# https://www.teesing.com/en/page/library/tools/ppm-mg3-converter
flux[,CO2_ppm := (co2_mean_Avg/0.0409)/44.01]
# get limit data #update with filters from the processing code
lims <- read.csv(paste0(basedir, "MpgVarLims.csv"))
# key for plotting multiple biomet in a single category
simpVar_key <- data.frame("OldVariable"=c("AirTC_6p85_Avg", "CNR1TC_Avg",
"RH_6p85_Avg",
"e_hmp_mean_Avg", "e_sat_Avg", "h2o_hmp_mean_Avg",
"Rad_short_Up_Avg", "Rad_short_Dn_Avg", "NetRs_Avg",
"Rad_long_Up_Avg", "Rad_long_Dn_Avg",
"NetRl_Avg", "CG3UpCo_Avg","CG3DnCo_Avg",
"wnd_dir_compass", "wnd_dir_csat3", "std_wnd_dir",
"rslt_wnd_spd", "wnd_spd",
"NetTot_Avg", "par_faceup_Avg",
"par_facedown_Avg",
"TargmV_Avg", "SBTempC_Avg", "TargTempC_Avg",
"P2_IRT_Avg" = "P2_IRT_Avg",
"P4_IRT_Avg" = "P4_IRT_Avg",
"P8_IRT_Avg" = "P8_IRT_Avg",
"P11_IRT_Avg" = "P11_IRT_Avg",
"J_1_IRT_Avg" = "J_1_IRT_Avg",
"J_2_IRT_Avg" = "J_2_IRT_Avg",
"J_3_IRT_Avg" = "J_3_IRT_Avg",
"J_4_IRT_Avg" = "J_4_IRT_Avg",
"canopyW_IRT_Avg" = "canopyW_IRT_Avg",
"canopyE_IRT_Avg" = "canopyE_IRT_Avg"),
"CombinedPlotVar"=c(rep("AirTC", 2), rep("RH", 1), rep("rH_hmp", 3), rep("SW", 3),
rep("LW", 3), rep("LW",2), rep("wind dir.", 3), rep("calcWS", 2),
"NetTot_Avg", rep("PAR", 2),
rep("infrared", 3), rep("IRT", 10)))
simpVar_key
# key for plotting multiple biomet in a single category
simpVar_key <- data.frame("OldVariable"=c("AirTC_1p5_Avg","AirTC_4p5_Avg","AirTC_8p75_Avg","AirTC_16_Avg",
"AirTC_24_Avg", "CNR1TC_Avg",
"RH_1p5_Avg", "RH_4p5_Avg", "RH_8p75_Avg", "RH_16_Avg",
"RH_24_Avg",
"e_hmp_mean_Avg", "e_sat_Avg", "h2o_hmp_mean_Avg",
"Rad_short_Up_Avg", "Rad_short_Dn_Avg",
"NetRs_Avg", "Rad_long_Up_Avg", "Rad_long_Dn_Avg",
"NetRl_Avg", "CG3UpCo_Avg","CG3DnCo_Avg",
"wnd_dir_compass", "wnd_dir_csat3", "std_wnd_dir", "W_dir_4p5",
"STD_W_dir_4p5",
"wnd_spd","rslt_wnd_spd","WS_ms_4p5",
"NetTot_Avg", "par_faceup_Avg",
"par_facedown_Avg", "TargmV_Avg", "SBTempC_Avg", "TargTempC_Avg",
"Soil Heat Flux 1 (open) (W/m2)" = "shf_p1_open_Avg",
"Soil Heat Flux 2 (open) (W/m2)" = "shf_p2_open_Avg",
"Soil Heat Flux 3 (tree) (W/m2)" = "shf_p3_tree_Avg"),
"CombinedPlotVar"=c(rep("AirTC", 6), rep("RH", 5), rep("rH_hmp", 3), rep("SW", 3),
rep("LW", 3),rep("LW",2),rep("wind dir.", 5),rep("calcWS", 3),
"NetTot_Avg", rep("PAR", 2),
rep("infrared", 3), rep("soil heat flux", 3)))
simpVar_key
# key for plotting multiple biomet in a single category
simpVar_key <- data.frame("OldVariable"=c("AirTC_6p85_Avg", "CNR1TC_Avg",
"RH_6p85_Avg",
"e_hmp_mean_Avg", "e_sat_Avg", "h2o_hmp_mean_Avg",
"Rad_short_Up_Avg", "Rad_short_Dn_Avg", "NetRs_Avg",
"Rad_long_Up_Avg", "Rad_long_Dn_Avg",
"NetRl_Avg", "CG3UpCo_Avg","CG3DnCo_Avg",
"wnd_dir_compass", "wnd_dir_csat3", "std_wnd_dir",
"rslt_wnd_spd", "wnd_spd",
"NetTot_Avg", "par_faceup_Avg",
"par_facedown_Avg",
"TargmV_Avg", "SBTempC_Avg", "TargTempC_Avg",
"P2_IRT_Avg" = "P2_IRT_Avg",
"P4_IRT_Avg" = "P4_IRT_Avg",
"P8_IRT_Avg" = "P8_IRT_Avg",
"P11_IRT_Avg" = "P11_IRT_Avg",
"J_1_IRT_Avg" = "J_1_IRT_Avg",
"J_2_IRT_Avg" = "J_2_IRT_Avg",
"J_3_IRT_Avg" = "J_3_IRT_Avg",
"J_4_IRT_Avg" = "J_4_IRT_Avg",
"canopyW_IRT_Avg" = "canopyW_IRT_Avg",
"canopyE_IRT_Avg" = "canopyE_IRT_Avg"),
"CombinedPlotVar"=c(rep("AirTC", 2), rep("RH", 1), rep("rH_hmp", 3), rep("SW", 3),
rep("LW", 3), rep("LW",2), rep("wind dir.", 3), rep("calcWS", 2),
"NetTot_Avg", rep("PAR", 2),
rep("infrared", 3), rep("IRT", 10)))
# import data directly from google drive
flux_colnames <- fread(paste0(basedir,"/toa5/flux.csv"),
header = TRUE, sep=",", fill=TRUE,
na.strings=c(-9999,"#NAME?"))[1,]
flux_colnames
fluxP
knitr::opts_chunk$set(echo = TRUE)
# load required libraries
library(flexdashboard)
library(data.table)
library(clifro)
library(openair)
library(ggplot2)
library(stringr)
library(DT)
library(gridExtra)
library(lubridate)
library(plotly)
library(tidyr)
library(shiny)
library(units)
library(ggpubr)
library(dplyr)
# ENTER YEAR TO CHECK HERE
### Beginning of time takes a really long time to render - have it as a separate script if someone wants to view it.
yeartocheck <- 2025
# ENTER SITE TO CHECK HERE (capital first letter, lowercase after)
site <- "Vcm"
# #basedir <- paste0("C:/FluxTowerData/", site, "/") # for Jakes computer
basedir <- paste0("~/Desktop/FluxTowerCheck/", site, "/")
# Create the directory if it doesn't exist
if (!dir.exists(basedir)) {
dir.create(basedir, recursive = TRUE)
}
setwd(basedir)
## these are for Vcp, add a loop to fetch for other sites - RD
file_ids <- c("15CAusJQddgR3RV3Y0kUwLlmUPpSLGLuV")  # file ID from google drive
file_names <- c(paste0(basedir, "toa5/flux.csv"))
# Create the directory if it doesn't exist
if (!dir.exists(paste0(basedir, "/toa5"))) {
dir.create(paste0(basedir, "/toa5"), recursive = TRUE)
}
# # Loop through file IDs to download
# for (i in seq_along(file_ids)) {
#      file_url <- paste0("https://drive.google.com/uc?id=", file_ids[i])
#       destfile <- file_names[i]
#       # Check if the destination file exists, and remove it if so
#       if (file.exists(destfile)) {
#         file.remove(destfile)
#       }
#       download.file(file_url, destfile, mode = "wb", timeout =500)
# }
# import data directly from google drive
flux_colnames <- fread(paste0(basedir,"/toa5/flux.csv"),
header = TRUE, skip=1,sep=",", fill=TRUE,
na.strings=c(-9999,"#NAME?"))[1,]
# fix units
flux_units <- flux_colnames[1,]
flux <- fread(paste0(basedir,"/toa5/flux.csv"),
header = FALSE, skip=2, sep=",", fill=TRUE,
na.strings=c(-9999,"#NAME?"),
col.names=colnames(flux_colnames))
flux <- read.csv(paste0(basedir, "/toa5/flux.csv"), header = FALSE, skip = 4)
colnames(flux) <- colnames(flux_colnames)
flux <- data.table(flux)
#convert to numeric
flux[, 2:ncol(flux) := lapply(.SD, function(x) {
as.numeric(as.character(x))
}), .SDcols = 2:ncol(flux)]
#convert timestamp to posix
flux$TIMESTAMP <- format(as.POSIXct(flux$TIMESTAMP, tz="MST"), format="%Y-%m-%d %H:%M:%S")
flux <- subset(flux, year(flux$TIMESTAMP) %in% yeartocheck) # change here
# create derivative date columns
flux[,':=' (year = year(TIMESTAMP), doy = yday(TIMESTAMP), date = as.Date(TIMESTAMP))]
flux$date_time = ymd_hms(flux$TIMESTAMP)
flux$date <- as.Date.POSIXct(flux$date_time, format="%Y-%m-%d")
# calculate CO2 ppm from CO2_mean (mg/m^3)
# https://www.teesing.com/en/page/library/tools/ppm-mg3-converter
flux[,CO2_ppm := (co2_mean_Avg/0.0409)/44.01]
# get limit data #update with filters from the processing code
lims <- read.csv(paste0(basedir, "VcmVarLims.csv"))
# grab soil data columns
## currently set up for Vcp
swc_columns <- c(grep("^SWC_", names(flux), value = TRUE), "TIMESTAMP")
soilT_columns <- c(grep("^SoilT", names(flux), value=TRUE), "TIMESTAMP")
soilC_columns <- c(grep("^Soil_CO2", names(flux), value=TRUE), "TIMESTAMP")
soil_SWC <- flux[, ..swc_columns]
soil_t <- flux[, ..soilT_columns]
soil_C <- flux[, ..soilC_columns]
# convert to long format and add metric, pit and depth columns
soilSWC <- soil_SWC %>%
pivot_longer(-c(TIMESTAMP),names_to="IDcol") %>%
separate(IDcol, c("metric","pit","depth"), sep="_")
soilT <- soil_t %>%
pivot_longer(-c(TIMESTAMP),names_to="IDcol") %>%
separate(IDcol, c("metric","pit","depth"), sep="_")
soilCO2 <- soil_C %>%
pivot_longer(-c(TIMESTAMP),names_to="IDcol") %>%
separate(IDcol, c(NA, "metric","pit","depth"), sep="_")
#bind new data together
soil <- rbind(soilSWC, soilT, soilCO2)
# format date/time and create depth labels for probes
soil <- soil %>%
mutate(date_time = ymd_hms(TIMESTAMP),
date = as.Date(date_time))
## create in range only columns for fluxes and IRGA (co2 & h2o) data
flux$Fc_wpl_IR <- ifelse(flux$Fc_wpl < -50 | flux$Fc_wpl >50, NA, flux$Fc_wpl)
flux$Hc_IR <- ifelse(flux$Hc < -500 | flux$Hc > 800, NA, flux$Hc)
flux$LE_wpl_IR <- ifelse(flux$LE_wpl < -150 | flux$LE_wpl > 300, NA, flux$LE_wpl)
flux$co2_um_m_Avg_IR <- ifelse(flux$co2_um_m_Avg < 400 | flux$co2_um_m_Avg > 460, NA, flux$co2_um_m_Avg)
flux$h2o_mm_m_Avg_IR <- ifelse(flux$h2o_mm_m_Avg < 0 | flux$h2o_mm_m_Avg > 100, NA, flux$h2o_mm_m_Avg)
## create tables with long runs (4 hours + of NAs. - more than 8 consecutive rows)
# first filter to prior 2 weeks # check here
flux1 <- flux %>%
filter(date >= (max(flux$date)-weeks(2)) & date <= (max(flux$date)))
# Make sure data is ordered by time
c_flux_with_na_runs <- flux1 %>%
arrange(date_time) %>%
mutate(is_na = is.na(Fc_wpl_IR))
# Run-length encoding
Fc_rle_na <- rle(c_flux_with_na_runs$is_na)
# Convert to a data frame
Fc_na_runs_df <- tibble(
run_length = Fc_rle_na$lengths,
is_na = Fc_rle_na$values
) %>%
mutate(
end_index = cumsum(run_length),
start_index = end_index - run_length + 1
) %>%
filter(is_na == TRUE & run_length >= 8)  # 8 half-hour intervals = 4 hours
# Pull actual datetimes from original data
Fc_long_na_periods <- Fc_na_runs_df %>%
rowwise() %>%
mutate(
start_time = c_flux_with_na_runs$date_time[start_index],
end_time   = c_flux_with_na_runs$date_time[end_index]
) %>%
select(start_time, end_time)
## Hc long NA runs.
# Make sure data is ordered by time
Hc_flux_with_na_runs <- flux1 %>%
arrange(date_time) %>%
mutate(is_na = is.na(Hc_IR))
# Run-length encoding
Hc_rle_na <- rle(Hc_flux_with_na_runs$is_na)
# Convert to a data frame
Hc_na_runs_df <- tibble(
run_length = Hc_rle_na$lengths,
is_na = Hc_rle_na$values
) %>%
mutate(
end_index = cumsum(run_length),
start_index = end_index - run_length + 1
) %>%
filter(is_na == TRUE & run_length >= 8)  # 8 half-hour intervals = 4 hours
# Pull actual datetimes from original data
Hc_long_na_periods <- Hc_na_runs_df %>%
rowwise() %>%
mutate(
start_time = Hc_flux_with_na_runs$date_time[start_index],
end_time   = Hc_flux_with_na_runs$date_time[end_index]
) %>%
select(start_time, end_time)
# repeat with LE
LE_flux_with_na_runs <- flux1 %>%
arrange(date_time) %>%
mutate(is_na = is.na(LE_wpl_IR))
# Run-length encoding
LE_rle_na <- rle(LE_flux_with_na_runs$is_na)
# Convert to a data frame
LE_na_runs_df <- tibble(
run_length = LE_rle_na$lengths,
is_na = LE_rle_na$values
) %>%
mutate(
end_index = cumsum(run_length),
start_index = end_index - run_length + 1
) %>%
filter(is_na == TRUE & run_length >= 8)  # 8 half-hour intervals = 4 hours
# Pull actual datetimes from original data
LE_long_na_periods <- LE_na_runs_df %>%
rowwise() %>%
mutate(
start_time = LE_flux_with_na_runs$date_time[start_index],
end_time   = LE_flux_with_na_runs$date_time[end_index]
) %>%
select(start_time, end_time)
### CO2 concentration in range
# Make sure data is ordered by time
co2_with_na_runs <- flux1 %>%
arrange(date_time) %>%
mutate(is_na = is.na(co2_um_m_Avg_IR))
# Run-length encoding
CO2_rle_na <- rle(co2_with_na_runs$is_na)
# Convert to a data frame
CO2_na_runs_df <- tibble(
run_length = CO2_rle_na$lengths,
is_na = CO2_rle_na$values
) %>%
mutate(
end_index = cumsum(run_length),
start_index = end_index - run_length + 1
) %>%
filter(is_na == TRUE & run_length >= 8)  # 8 half-hour intervals = 4 hours
# Pull actual datetimes from original data
CO2_long_na_periods <- CO2_na_runs_df %>%
rowwise() %>%
mutate(
start_time = co2_with_na_runs$date_time[start_index],
end_time   = co2_with_na_runs$date_time[end_index]
) %>%
select(start_time, end_time)
### H2O concentration in range
# Make sure data is ordered by time
h2o_with_na_runs <- flux1 %>%
arrange(date_time) %>%
mutate(is_na = is.na(h2o_mm_m_Avg_IR))
# Run-length encoding
h2o_rle_na <- rle(h2o_with_na_runs$is_na)
# Convert to a data frame
h2o_na_runs_df <- tibble(
run_length = h2o_rle_na$lengths,
is_na = h2o_rle_na$values
) %>%
mutate(
end_index = cumsum(run_length),
start_index = end_index - run_length + 1
) %>%
filter(is_na == TRUE & run_length >= 8)  # 8 half-hour intervals = 4 hours
# Pull actual datetimes from original data
h2o_long_na_periods <- h2o_na_runs_df %>%
rowwise() %>%
mutate(
start_time = h2o_with_na_runs$date_time[start_index],
end_time   = h2o_with_na_runs$date_time[end_index]
) %>%
select(start_time, end_time)
print(max(flux$TIMESTAMP, na.rm = T))
flux$TIMESTAMP
flux
